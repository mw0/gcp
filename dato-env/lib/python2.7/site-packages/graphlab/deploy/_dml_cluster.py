import os as _os
import json as _json
import time as _time
import shutil as _shutil
import logging as _logging
import requests as _requests
import tempfile as _tempfile
from uuid import uuid4 as _uuid
from datetime import datetime as _datetime

from graphlab_util import file_util as _file_util
from graphlab.deploy._executionenvironment import \
    ExecutionEnvironment as _ExecutionEnvironment

from environment import _Environment
import _job

_log = _logging.getLogger(__name__)


def create(environment, timeout_in_seconds = 60 * 10):
    '''Create a new DML Execution Engine

    Parameters
    ----------
    environment : Ec2Cluster | HadoopCluster
        The environment to create the DML Execution Engine in. This is either
        a Hadoop Cluster or Ec2 Cluster

    timeout_in_seconds : int, optional
        Timeout in seconds. The execution context will shutdown automatically if
        it is in idle for given timeout. If not given, the default value is 10
        minutes (600 seconds). After the execution context timed out, it will
        automatically be spinned up again next time you submit job.
    '''

    _log.info('Setting up distributed execution context...')

    dml_cluster = DmlExecutionEngine(environment, timeout_in_seconds)
    dml_cluster.start()
    return dml_cluster

class DmlExecutionEngine(_Environment):

    # how long do we wait before we cancel the cluster launch
    _LAUNCH_TIMEOUT = 10 * 60

    def __init__(self, environment, timeout_in_seconds):
        '''
        Initializes a new DML Execution Engine for submitting DML job
        '''
        assert environment is not None
        self.environment = environment
        self.cluster_controller = None
        self.cluster_working_dir = None
        self.app_id = None
        self.timeout_in_seconds = timeout_in_seconds

        super(DmlExecutionEngine, self).__init__('DML Execution Engine', session_aware=False)

    def __del__(self):
        try:
            if self.cluster_controller:
                self.stop()
        except Exception as e:
            _log.error(e)

    def __str__(self):
        ret = 'DML Execution Engine:\n'
        ret += '\tApplication id      : %s\n' % self.app_id

        if self.cluster_controller:
            ret += '\tStatus              : Running\n'
            ret += '\tEndpoint            : %s\n' % self.cluster_controller
        else:
            ret += '\tStatus              : Stopped\n'

        return ret

    def is_running(self):
        '''
        Check whether or not the DML Execution Engine is running

        It is considerred running if all of the following conditions are true:
          1. There was a DML job started (cluster_controller is not None)
          2. The DML job is running (can talk to commander)
          3. All workers are healthy (the commander is in Ready state)

        '''
        if self.cluster_controller is None:
            return False

        # get_status returns when the commander is responsing
        status = self.get_status(_silent = True)
        if status is None:
            return False

        # all worker has to be up and running in order for cluster to be in
        # ready state
        if status['status'] != 'Ready':
            return False

        return True

    def start(self):
        if self.is_running():
            raise RuntimeError('DML Execution Engine already started, use stop() to stop it.')

        environment = self.environment
        cluster_name = 'cluster_%s' % str(_uuid())
        self.cluster_working_dir = environment._create_job_home_dir(cluster_name)

        # put initial status file and metadata files
        self._prepare_cluster_init_files()

        # Wait for the application to start and then retrieve commander port
        try:
            _log.info('Step 1/4: submitting the engine application...')

            # submit actual application
            self.app_id = environment._submit_job(
                job_working_dir = self.cluster_working_dir,
                num_workers = environment.get_num_workers(),
                silent = True)

            _log.info('Step 2/4: waiting for the engine(%s) to run ...' % self.app_id)
            self._wait_for_application_start()

            # Read commander init file to get commander listening URI
            _log.info('Step 3/4: waiting for commander to be ready...')
            commander_url = self._wait_for_commander_ready()
            self.cluster_controller = commander_url

            # Wait for all workers to be ready
            _log.info('Step 4/4: waiting for all %s workers to be ready...' % environment.get_num_workers())
            self._wait_for_all_workers_ready()

            _log.info('Cluster is listening at: %s' % commander_url)
        except Exception as e:
            _log.error('Error encountered when waiting for DML Execution Engine to start: %s' % e)

            if self.app_id:
                environment._cancel_job(self.app_id, silent = True)
            raise

    def stop(self):
        '''
        Stop the DML Execution Engine
        '''
        if not self.cluster_controller:
            _log.warning('There is no active DML Execution Engine to stop.')
        else:
            success = self.environment._cancel_job(self.app_id, silent = True)

            if success:
                _log.info('DML Execution Engine stopped successfully')
            else:
                _log.warning('DML Execution Engine cannot be stopped. Check '
                             'previous error for more information.')

        self.cluster_controller = None

    def get_num_workers(self):
        return self.environment.get_num_workers()

    def get_status(self, _silent = False):
        '''
        Get DML Execution Engine status

        Returns
        --------
        status : dict
            A dictionary indicate current DML Execution Engine status.
            If there is error connecting to the engine, returns None

            Possible statuses for the engine are:
                * Pending -- waiting for all worker to be ready
                * Ready -- engine is ready to process DML job
                * Running -- engine is processing a DML job
                * Stopped -- engine is not started or has stopped

        An example return value would be:
            {
                'status':'Running',
                'message':'Procesing DML job',
                'num_workers': 3,
                'num_req_worker': 3
            }
        '''
        if not self.cluster_controller:
            return {'status': 'stopped'}
        else:
            try:
                resp = _requests.get('%s/get_status' % self.cluster_controller)
                if not resp:
                    if not _silent:
                        _log.error('Cannot get DML Execution Engine status: %s' % resp.text)
                    return None
                else:
                    return resp.json()
            except Exception as e:
                if not _silent:
                    _log.info('Error connecting to DML engine, it may have timed out. %s' % e)

            return None

    def run_job(self, job):
        assert job._job_type == 'DML'
        assert job.environment == self

        if not self.is_running():
            _log.info('DML Execution Engine is either timed out or is not in a '
                      'healthy state, restarting...')
            self.stop()
            self.start()

        self.environment._prepare_job_files(job)

        _log.info("Submitting DML job...")
        cmd_args = {'job_working_dir': job._exec_dir}
        resp = _requests.post('%s/submit' % job.environment.cluster_controller, params = cmd_args)
        if not resp:
            raise RuntimeError('Cannot submit job to cluster. Response: %s.' % resp.json())

        return _job.DmlJob(job)

    def cancel_job(self):
        '''
        Cancel current running job

        TODO -- hook up with CTRL+C in DML code to cancel a job
        '''
        if not self.cluster_controller:
            _log.warning('Cluster is not running, nothing to cancel')
            return False
        else:
            resp = _requests.post('%s/cancel' % self.cluster_controller)
            if resp.status_code == 200:
                return True
            else:
                _log.error('Cannot cancel job: %s' % resp.text)
                return None

    def _get_commander_file_path(self):
        return self.cluster_working_dir + '/' + 'commander_init.status'

    def _prepare_cluster_init_files(self):
        '''
        Write metadata and commander init file
        We write to local first and then upload to remote
        '''
        tmp_dir = _tempfile.mkdtemp(prefix='dml_cluster_')
        try:
            cmd_init_status_file = _os.path.join(tmp_dir, 'commander_init.status')
            metadata_file = _os.path.join(tmp_dir, 'metadata')
            _os.mkdir(_os.path.join(tmp_dir, 'logs'))

            _ExecutionEnvironment._write_commander_init_file(cmd_init_status_file)

            # write dml job metadata
            metadata = {
                'job_type': 'DML',
                'job_working_dir': self.cluster_working_dir,
                'timeout_in_seconds': self.timeout_in_seconds
            }
            with open(metadata_file, 'w') as f:
                _json.dump(metadata, f)

            # move to remote
            self._upload_folder_to_remote(tmp_dir, self.cluster_working_dir)
        finally:
            if _os.path.isdir(tmp_dir):
                _shutil.rmtree(tmp_dir)

    def _wait_for_application_start(self):
        '''
        Wait for the Yarn application to enter RUNNING state.

        Raise exception if the application is failed or it takes too long
        to start the DML Execution Engine.

        '''
        started = _datetime.now()
        while ((_datetime.now() - started).total_seconds() < self._LAUNCH_TIMEOUT):
            yarn_app_states = self._fail_if_yarn_app_finished()

            am_state = yarn_app_states['AppState']
            if am_state == 'RUNNING':
                return

            _time.sleep(1)

        raise RuntimeError('It has been too long to start a DML Execution Engine.')

    def _wait_for_commander_ready(self):
        '''
        Wait for commander to be ready

        '''
        started = _datetime.now()
        while ((_datetime.now() - started).total_seconds() < self._LAUNCH_TIMEOUT):
            commander_url = self._read_commander_init_status_file()
            if commander_url is not None:
                return commander_url

            self._fail_if_yarn_app_finished()

            _time.sleep(1)

        raise RuntimeError('It has been too long to get commander ready, cancel it now.')

    def _fail_if_yarn_app_finished(self):
        # Keep checking Yarn job status too, as the Yarn may failed already
        yarn_app_states = self.environment._get_job_state(self.app_id, silent=True)
        if not yarn_app_states:
            raise RuntimeError('Cannot get application status from cluster. '
                                'Please check if cluster is in healthy state.')

        final_state = yarn_app_states['DistributedFinalState']
        if final_state != 'UNDEFINED':
            raise RuntimeError(
                'DML execution engine terminated unexpectedly, final state: %s.'
                % (final_state))

        return yarn_app_states

    def _wait_for_all_workers_ready(self):
        started = _datetime.now()
        while ((_datetime.now() - started).total_seconds() < self._LAUNCH_TIMEOUT):
            status = self.get_status(_silent = True)
            if status is None:
                raise RuntimeError('DML Execution Engine terminated unexpectedly.')

            if status['status'] == 'Ready':
                return

            self._fail_if_yarn_app_finished()
            _time.sleep(1)

        raise RuntimeError('Seems like there is not enough resource available to '
            'start a DML Execution Engine.')

    def _upload_folder_to_remote(self, local, remote):
        if _file_util.is_s3_path(remote):
            _file_util.upload_to_s3(
                local,
                remote,
                is_dir = True,
                aws_credentials = self.environment.get_credentials(),
                silent = True)
        elif _file_util.is_hdfs_path(remote):
            _file_util.upload_folder_to_hdfs(
                local,
                remote,
                self.environment.hadoop_conf_dir)

    def _read_commander_init_status_file(self):
        commander_file_path = self._get_commander_file_path()

        local_file_name = _tempfile.mktemp(prefix='dml_file_')
        try:
            if _file_util.is_hdfs_path(commander_file_path):
                _file_util.download_from_hdfs(
                    commander_file_path,
                    local_file_name,
                    hadoop_conf_dir = self.environment.hadoop_conf_dir)
            elif _file_util.is_s3_path(commander_file_path):
                _file_util.download_from_s3(
                    commander_file_path,
                    local_file_name,
                    aws_credentials = self.environment.get_credentials(),
                    silent = True)

            with open(local_file_name,'r') as f:
                status_json = _json.load(f)
                port = status_json['port']
                host_name = status_json['host_name']

            if port > 0:
                return 'http://%s:%s' % (host_name, port)
            else:
                return None
        except:
            # Ignore exception, we will fail after a few retry
            pass
        finally:
            if _os.path.exists(local_file_name):
                _os.remove(local_file_name)
