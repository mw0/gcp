import tempfile
import os
from uuid import uuid4 as uuid
import graphlab as gl
import logging
from graphlab.deploy import _dml_job

"""
Define DML Job Environment classes.

Any dml job environment object contains the following attributes:
    job_id : str
      unique identifier of the dml job
    working_dir : str
      directory for storing job related serializations and logs
    num_workers : int
      number of job workers
    output_name : str
      name of the output file for storing the job output

It also implements a submit function, which takes a function
and list of map arguments and return a map job object.
"""


def change_job_logger_level(loglevel):
    """
    Decorator to raise the job related logger to warning while executing f
    """
    def wrap(f):
        def decorated(*args):
            loggers = [logging.getLogger('graphlab.deploy.job'),
                       logging.getLogger('graphlab.deploy.map_job'),
                       logging.getLogger('graphlab.deploy.dml_job'),
                       logging.getLogger('graphlab.deploy._job'),
                       logging.getLogger('graphlab.deploy._dml_cluster')]
            old_levels = [l.level for l in loggers]
            for l in loggers:
                l.setLevel(loglevel)
            try:
                return f(*args)
            finally:
                for i in range(len(loggers)):
                    loggers[i].setLevel(old_levels[i])
        return decorated
    return wrap


class DMLLocalEnvironment(object):
    SCRIPT_DIR = os.path.dirname(os.path.realpath(__file__))
    DML_DIR = SCRIPT_DIR
    COMMANDER_PATH = os.path.join(DML_DIR, 'dml_commander_startup')
    WORKER_PATH = os.path.join(DML_DIR, 'dml_worker_startup')
    LIB_UNITY_DISTRIBUTED_PATH = os.path.join(DML_DIR, 'libunity_distributed.so')

    def __init__(self, num_workers='auto',
                 job_id='auto', output_name='out'):
        if job_id == 'auto':
            self.job_id = str(uuid())
        else:
            self.job_id = job_id

        self.working_dir = os.path.join(tempfile.gettempdir(), 'job_' + self.job_id)
        self.output_name = output_name

        if num_workers == 'auto':
            self.num_workers = 2
        else:
            self.num_workers = num_workers

    @change_job_logger_level(logging.WARNING)
    def submit(self, f, list_of_map_args):
        """
        LocalAsync map job runs sequentially, but the dml job depend
        on parallel execution of commander and workers,
        as a workaround we simulate a map job using jobs
        """
        jobs = []
        for argmap in list_of_map_args:
            name = f.__name__ + str(uuid())
            jobs.append(gl.deploy.job.create(f, name=name, **argmap))

        # A list of lambdas, on calling each will return the results of job i
        f = lambda job: job.get_results()
        name = "lambda-get-results-%s" % str(uuid())
        map_job = gl.deploy.map_job.create(f, [{'job': j} for j in jobs], name = name)
        return map_job


class DMLRemoteEnvironment(object):
    REMOTE_DML_DIR = 'dato/bins/pipeline/dml/'
    COMMANDER_PATH = os.path.join(REMOTE_DML_DIR, 'dml_commander_startup')
    WORKER_PATH = os.path.join(REMOTE_DML_DIR, 'dml_worker_startup')
    LIB_UNITY_DISTRIBUTED_PATH = os.path.join(REMOTE_DML_DIR, 'libunity_distributed.so')

    def __init__(self, num_workers='auto', job_id='auto', output_name='out'):
        self._env = gl._distributed_execution_environment.get_distributed_execution_environment()
        if self._env is None:
            raise RuntimeError('Please use graphlab.set_distributed_execution_environment() to set distributed execution environment first.')

        if job_id == 'auto':
            self.job_id = str(uuid())
        else:
            self.job_id = job_id

        self.working_dir = os.path.join(self._env.cluster_working_dir, 'job_' + self.job_id)
        self.output_name = output_name

        max_workers = self._env.get_num_workers() - 1
        if max_workers <= 0:
            raise RuntimeError('Execution environment must have at least 2 workers')

        if num_workers == 'auto':
            self.num_workers = max_workers
        elif num_workers >= max_workers:
            raise ValueError('num_workers exceeds limit: %d' % max_workers)

    @change_job_logger_level(logging.WARNING)
    def submit(self, f, list_of_map_args):
        return _dml_job.create(f, list_of_map_args)
